version: "3.8"
services:
  pgsql-spark:
    image: pgsql-spark
    build:
      context: .
      dockerfile: Dockerfile-postgresql
    ports:
        - 5432:5432
    environment:
        - POSTGRES_USER=sa
        - POSTGRES_PASSWORD=pwd
        - POSTGRES_DB=psycodb
  stage_data:
    image: stage_data
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 4042:4040
      - 10000:10000
    command: [
      "/bin/sh",
      "-c",
      "/opt/spark/bin/spark-submit \
      --name stage_data \
      --conf spark.jars.packages=io.delta:delta-core_2.12:1.0.0 \
      --conf spark.jars.ivy=/tmp/.ivy \
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
      local:///opt/spark/stage_data/main.py \
      acquire-vitals \
      --filepath /opt/spark/work-dir/input_data/raw/healthjump/vitals/2021/10/15/aledade_vitals_rh_20211015.gz
      --filepath2 /opt/spark/work-dir/input_data/raw/healthjump/vitals/2021/10/18/aledade_vitals_rh_20211018.gz
      --output-path /opt/spark/work-dir/output_data"
    ]
    depends_on:
      - pgsql-spark
    env_file: .env
    environment:
      - LOG_LEVEL=INFO
      - SPARK_LOG_LEVEL=WARN
      - POSTGRES_DB_URL=postgresql://sa:pwd@pgsql-spark/psycodb
      - POSTGRES_JDBC_URL=jdbc:postgresql://pgsql-spark/psycodb?user=sa&password=pwd
    volumes:
      - ./local_config:/opt/spark/conf
      # - ./local_hive:/opt/spark/hive_warehouse
      - ./sample_data/input_data:/opt/spark/work-dir/input_data
      - ./sample_data/output_data:/opt/spark/work-dir/output_data
  jupyter-spark:
    image: jupyter-spark
    build:
      context: .
      dockerfile: Dockerfile-notebook
    ports:
      - 8888:8888
    command: "jupyter notebook --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.token=''"
    depends_on:
      - pgsql-spark
    env_file: .env
    environment:
      - POSTGRES_DB_URL=postgresql://sa:pwd@pgsql-spark/psycodb
    volumes:
      - ./notebooks:/opt/spark/jupyter-lib/notebooks
      - ./sample_data/output_data:/opt/spark/jupyter-lib/output_data
